{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.agents import tool\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "import os\n",
    "\n",
    "\n",
    "OPEN_AI_API_KEY = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "\n",
    "import snowflake.connector as connector\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import json\n",
    "from typing import Iterable, Union\n",
    "import enum\n",
    "\n",
    "class SyrupSnowflakeWarehouse(str, enum.Enum):\n",
    "    PROD_LOAD_WAREHOUSE = \"prod_load_warehouse\"\n",
    "    PROD_DBT_WAREHOUSE = \"prod_dbt_warehouse\"\n",
    "    PROD_DASHBOARD_WAREHOUSE = \"prod_dashboard_warehouse\"\n",
    "    PROD_DEFAULT_WAREHOUSE = \"prod_default_warehouse\"\n",
    "\n",
    "\n",
    "class SyrupSnowflakeConnection:\n",
    "    def __init__(self, user: str, password: str, account: str):\n",
    "        self._account: str = account\n",
    "        self._user: str = user\n",
    "        self._password: str = password\n",
    "\n",
    "    @classmethod\n",
    "    def from_secret(cls, secret_id: str):\n",
    "        \"\"\"\n",
    "        Builds a connection using credentials stored in AWS Secrets.\n",
    "\n",
    "        Secret must be a JSON string with keys \"user\", \"password\", and \"account\".\n",
    "\n",
    "        :param secret_id: The ID of the secret in AWS.\n",
    "        :type secret_id: str\n",
    "        \"\"\"\n",
    "        secret = json.loads(\n",
    "            boto3.client(\"secretsmanager\").get_secret_value(SecretId=secret_id)[\n",
    "                \"SecretString\"\n",
    "            ]\n",
    "        )\n",
    "        user = secret[\"user\"]\n",
    "        password = secret[\"password\"]\n",
    "        account = secret[\"account\"]\n",
    "        return cls(user, password, account)\n",
    "\n",
    "    @contextlib.contextmanager\n",
    "    def connection(\n",
    "        self, database: str = None, schema: str = None, warehouse: Union[SyrupSnowflakeWarehouse, str] = None, **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Opens a connection to Snowflake using the `contextmanager` pattern using the credentials\n",
    "        in config.py.\n",
    "\n",
    "        :param database: The Snowflake database queries will be executed in.\n",
    "        :type database: str\n",
    "        :param schema: The Snowflake schema queries will be executed in -- must exist in the specified database.\n",
    "        :type schema: str\n",
    "        :param warehouse: The warehouse to run the query.\n",
    "        :type warehouse: str\n",
    "        \"\"\"\n",
    "        if warehouse is None:\n",
    "            warehouse = SyrupSnowflakeWarehouse.PROD_DEFAULT_WAREHOUSE.value\n",
    "        else:\n",
    "            if isinstance(warehouse, SyrupSnowflakeWarehouse):\n",
    "                warehouse = warehouse.value\n",
    "\n",
    "        conn = connector.connect(\n",
    "            database=database,\n",
    "            schema=schema,\n",
    "            warehouse=warehouse,\n",
    "            password=self._password,\n",
    "            user=self._user,\n",
    "            account=self._account,\n",
    "            **kwargs,\n",
    "        )\n",
    "        try:\n",
    "            yield conn\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    @contextlib.contextmanager\n",
    "    def cursor(self, database: str = None, schema: str = None, warehouse: str = None):\n",
    "        \"\"\"\n",
    "        Opens a connection to Snowflake and returns a cursor using the `contextmanager` pattern.\n",
    "\n",
    "        :param database: The Snowflake database queries will be executed in.\n",
    "        :type database: str\n",
    "        :param schema: The Snowflake schema queries will be executed in -- must exist in the specified database.\n",
    "        :type schema: str\n",
    "        :param warehouse: The warehouse to run the query.\n",
    "        :type warehouse: str\n",
    "        \"\"\"\n",
    "        with self.connection(\n",
    "            database=database, schema=schema, warehouse=warehouse\n",
    "        ) as conn:\n",
    "            cur = conn.cursor()\n",
    "            try:\n",
    "                yield cur\n",
    "            finally:\n",
    "                cur.close()\n",
    "\n",
    "    def fetch_pandas_all(\n",
    "        self,\n",
    "        query: str,\n",
    "        database: str = None,\n",
    "        schema: str = None,\n",
    "        warehouse: str = None,\n",
    "        lower_case_cols: bool = True,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Executes a query against Snowflake and returns the full result as a Pandas DataFrame.\n",
    "\n",
    "        :param query: The SQL query to execute.\n",
    "        :type query: str\n",
    "        :param database: The Snowflake database queries will be executed in.\n",
    "        :type database: str\n",
    "        :param schema: The Snowflake schema queries will be executed in -- must exist in the specified database.\n",
    "        :type schema: str\n",
    "        :param warehouse: The warehouse to run the query.\n",
    "        :type warehouse: str\n",
    "        \"\"\"\n",
    "        with self.cursor(database=database, schema=schema, warehouse=warehouse) as cur:\n",
    "            df = cur.execute(query).fetch_pandas_all()\n",
    "            if lower_case_cols:\n",
    "                df.columns = [col.lower() for col in df.columns]\n",
    "            return df\n",
    "\n",
    "    def fetch_pandas_batches(\n",
    "        self,\n",
    "        query: str,\n",
    "        database: str = None,\n",
    "        schema: str = None,\n",
    "        warehouse: str = None,\n",
    "    ) -> Iterable[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Executes a query against Snowflake and returns an Iterator of Pandas DataFrames.\n",
    "\n",
    "        :param query: The SQL query to execute.\n",
    "        :type query: str\n",
    "        :param database: The Snowflake database queries will be executed in.\n",
    "        :type database: str\n",
    "        :param schema: The Snowflake schema queries will be executed in -- must exist in the specified database.\n",
    "        :type schema: str\n",
    "        :param warehouse: The warehouse to run the query.\n",
    "        :type warehouse: str\n",
    "        \"\"\"\n",
    "        with self.cursor(database=database, schema=schema, warehouse=warehouse) as cur:\n",
    "            return cur.execute(query).fetch_pandas_batches()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "snowflake = SyrupSnowflakeConnection.from_secret(\"prod/syrup-snowflake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def sql_query(query):\n",
    "    \"\"\"\n",
    "    This tool is useful when running Snowflake queries leads to the answer to the given question.\n",
    "\n",
    "    You have access to the following Snowflake schemas and tables that describes the metrics of each demand forecasts generated for our clients:\n",
    "    In schema saheel_metrics there is a table metrics with columns: CLIENT FORECAST_ID MAE_BASELINE_4W_CONSERVATIVE MAE_BASELINE_4W_OPTIMISTIC MAE_Y_PRED NUM_WEEKS_EVALUATED PERC_MAE_BASELINE_4W_CONSERVATIVE PERC_MAE_BASELINE_4W_OPTIMISTIC PERC_MAE_Y_PRED PRED_WEEK TYPE_ID. CLIENT column contains the name of the client. FORECAST_ID is the id of the forecast. PRED_WEEK column contains the date of the week this forecast was generated. TYPE_ID corresponds to the type of forecast (1 is Reorders forecast, 2 is Replenishment forecast). NUM_WEEKS_EVALUATED column contains the number of weeks we have monitored this forecast before calculated the metrics. Other columns correspond to various metrics calculated for this forecast.\n",
    "    In schema saheel_sources there is a table faherty__recommended_shifts_backup with columns: APPROVED ART_NR CLIENT DATE FROM_STORE_NR INTERNAL_REVIEW SIZE TO_STORE_NR UNITS UNITS_FINAL. 'from_store_nr' and 'to_store_nr' represent id columns of the articles, destination and origin stores from which the shifts are happening. 'size' represents the clothes article size, date is the date of the week, and units colum represents number of units that are reccomended to be shifted from one store to another.  'approved' column represents info whether the shifts was 'pending', 'accepted', 'rejected'.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    query = query.replace('`', '')\n",
    "    return snowflake.fetch_pandas_all(query, database=\"syrup\", warehouse=\"saheel_warehouse\").to_markdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.document_loaders import BSHTMLLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "\n",
    "# loader = TextLoader(\"/Users/saheel/Downloads/single.md\")\n",
    "loader = BSHTMLLoader(\"/Users/saheel/Downloads/single.html\")\n",
    "data = loader.load()\n",
    "text_splitter = CharacterTextSplitter(separator = '\\n', chunk_size=1500, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(data)\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPEN_AI_API_KEY)\n",
    "docsearch = Chroma.from_documents(texts, embeddings)\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=OpenAI(openai_api_key=OPEN_AI_API_KEY), chain_type=\"stuff\", retriever=docsearch.as_retriever())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def retrieveQA(query):\n",
    "    \"\"\"\n",
    "    You must use this tool whenever the asked question is related to Syrup Tech\n",
    "    \"\"\"\n",
    "    return qa.run(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' You can exclude stores from replenishments by using the Exclude Stores setting when configuring Syrup. This will narrow down the replenishment recommendations to a certain set of stores.'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "retrieveQA(\"how can we exclude certain stores from replenishments?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(openai_api_key=OPEN_AI_API_KEY)\n",
    "tools = [\n",
    "    retrieveQA,\n",
    "    sql_query\n",
    "]\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=\"conversational-react-description\",\n",
    "    verbose=True,\n",
    "    max_iterations=2,\n",
    "    memory=memory,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3542"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(agent.agent.llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Thought: Do I need to use a tool? No\n",
      "AI: Hi Saheel, how can I help you?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hi Saheel, how can I help you?'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(input=\"Hi, this is Saheel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? No\n",
      "AI: Syrup Tech is a supply chain analytics platform that provides demand forecasting and replenishment solutions. It helps businesses better plan their inventory and optimize their supply chain operations. It uses advanced machine learning algorithms to enable predictive forecasting, identify inventory needs, and automate the replenishment process. Syrup Tech also provides powerful reporting tools and insights to help businesses understand their inventory levels and make informed decisions.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Syrup Tech is a supply chain analytics platform that provides demand forecasting and replenishment solutions. It helps businesses better plan their inventory and optimize their supply chain operations. It uses advanced machine learning algorithms to enable predictive forecasting, identify inventory needs, and automate the replenishment process. Syrup Tech also provides powerful reporting tools and insights to help businesses understand their inventory levels and make informed decisions.'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(input=\"what do you know about Syrup Tech?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Thought: Do I need to use a tool? No\n",
      "AI: You can exclude certain stores when running replenishments in Syrup Tech by using the retrieveQA tool. With this tool, you can create a query that retrieves only the stores that you would like to include or exclude from the replenishment.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'You can exclude certain stores when running replenishments in Syrup Tech by using the retrieveQA tool. With this tool, you can create a query that retrieves only the stores that you would like to include or exclude from the replenishment.'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"How can we exclude certain stores when running replenishments in Syrup Tech?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Thought: Do I need to use a tool? No\n",
      "AI: Syrup Tech offers a variety of integration options, such as automated data feeds, APIs, webhooks, and direct database integrations. This allows organizations to easily integrate their existing systems and data with Syrup Tech to maximize efficiency, accuracy, and scalability.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Syrup Tech offers a variety of integration options, such as automated data feeds, APIs, webhooks, and direct database integrations. This allows organizations to easily integrate their existing systems and data with Syrup Tech to maximize efficiency, accuracy, and scalability.'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"What are the various integration options provided by Syrup Tech\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Thought: Do I need to use a tool? No\n",
      "AI: Initial allocations in Syrup Tech can be done using the Initial Allocations module. This module allows you to specify the source and destination stores, the product to be allocated, and the quantity. You can also set business rules and restrictions to ensure that the allocations are accurate and follow your organization's specific requirements.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Initial allocations in Syrup Tech can be done using the Initial Allocations module. This module allows you to specify the source and destination stores, the product to be allocated, and the quantity. You can also set business rules and restrictions to ensure that the allocations are accurate and follow your organization's specific requirements.\""
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"How can we do Initial Allocations in Syrup Tech?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Thought: Do I need to use a tool? No\n",
      "AI: Syrup Tech uses a variety of file transfer strategies, including FTP/SFTP, S3, and direct database integrations. The file transfer method used depends on the customer's specific requirements and needs. All transfer strategies are secure, efficient, and designed to make sure that data is transferred quickly, accurately, and securely.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Syrup Tech uses a variety of file transfer strategies, including FTP/SFTP, S3, and direct database integrations. The file transfer method used depends on the customer's specific requirements and needs. All transfer strategies are secure, efficient, and designed to make sure that data is transferred quickly, accurately, and securely.\""
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"What File Transfer strategies are used by Syrup Tech?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Thought: Do I need to use a tool? No\n",
      "AI: Syrup Tech supports direct database integrations with PostgreSQL, MySQL, Oracle, and Microsoft SQL Server databases. With direct database integration, data can be easily transferred between your existing databases and Syrup Tech, providing an efficient and secure method of data exchange.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Syrup Tech supports direct database integrations with PostgreSQL, MySQL, Oracle, and Microsoft SQL Server databases. With direct database integration, data can be easily transferred between your existing databases and Syrup Tech, providing an efficient and secure method of data exchange.'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"And what databases can we connect to?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Thought: Do I need to use a tool? Yes\n",
      "Action: sql_query\n",
      "Action Input: SELECT * FROM saheel_sources.faherty__recommended_shifts_backup WHERE approved = 'accepted'\u001b[0m------------Running query--------------\n",
      "SELECT * FROM saheel_sources.faherty__recommended_shifts_backup WHERE approved = 'accepted'\n"
     ]
    }
   ],
   "source": [
    "agent.run(\"How do I accept a transfer?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "67bfac4f4aefe1c16f1836a62d55b6e6baa7aba1ac5ce70e93ee8e90eb4f073a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
